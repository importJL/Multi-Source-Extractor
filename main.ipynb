{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import re\n",
    "# from dotenv import load_dotenv\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from pprint import pprint\n",
    "import spacy\n",
    "import multiextractor\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alphanum_regex = re.compile(r'\\w+')\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_dict = {\n",
    "#     'general': 'general',\n",
    "#     'world': 'world',\n",
    "#     'nation': 'nation',\n",
    "#     'business': 'business',\n",
    "#     'technology': 'technology',\n",
    "#     'entertainment': 'entertainment',\n",
    "#     'sports': 'sports',\n",
    "#     'science': 'science',\n",
    "#     'health': 'health'\n",
    "# }\n",
    "# language_dict = {\n",
    "#     'Chinese': 'zh',\n",
    "#     'English': 'en'\n",
    "# }\n",
    "# country_dict = {\n",
    "#     'Australia': 'au',\n",
    "#     'Brazil': 'br',\n",
    "#     'Canada': 'ca',\n",
    "#     'Hong Kong': 'hk',\n",
    "#     'United Kingdom': 'gb',\n",
    "#     'United States': 'us'\n",
    "# }\n",
    "\n",
    "category = multiextractor.CATEGORY['technology']\n",
    "language = multiextractor.LANGUAGE['English']\n",
    "country = multiextractor.COUNTRY['United States']\n",
    "max_articles = 10\n",
    "\n",
    "# EXTRACT - articles\n",
    "data, articles = multiextractor.extract_news()\n",
    "total_articles = data['totalArticles']\n",
    "df_articles = pd.DataFrame.from_dict(articles)\n",
    "\n",
    "# url = f\"https://gnews.io/api/v4/top-headlines?category={category}&lang={language}&country={country}&max={max_articles}&apikey={GNEWS_KEY}\"\n",
    "\n",
    "# # data = None\n",
    "# with urllib.request.urlopen(url) as response:\n",
    "#     data = json.loads(response.read().decode(\"utf-8\"))\n",
    "#     articles = data[\"articles\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All articles inserted\n"
     ]
    }
   ],
   "source": [
    "# load prexisting data\n",
    "# if data is None:\n",
    "#     df_articles = pd.read_csv('sample_gnews_extracted_data.csv')\n",
    "\n",
    "### TRANSFORM - articles\n",
    "# df_articles = multiextractor.rename_columns(df_articles)\n",
    "df_articles = multiextractor.split_source(df_articles)\n",
    "df_articles = multiextractor.process_datetime(df_articles)\n",
    "df_articles = multiextractor.process_sentence_count(df_articles, nlp, 'title', 'description', 'content')\n",
    "df_articles = multiextractor.process_token_count(df_articles, nlp, 'title', 'description', 'content')\n",
    "\n",
    "### LOAD - articles\n",
    "# multiextractor.sql_insert_articles(df_articles)\n",
    "multiextractor.sql_create_table(df_articles, unique_col='title')\n",
    "multiextractor.sql_insert_articles(df_articles, constraint_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # split source column\n",
    "# df_source = df_articles['source'].apply(pd.Series).rename({'url': 'domainName'}, axis=1)\n",
    "# df_articles = pd.concat([df_articles, df_source], axis=1).drop('source', axis=1)\n",
    "\n",
    "# # convert to datetime format and extract dates\n",
    "# df_articles['publishedAt'] = pd.to_datetime(df_articles['publishedAt'])\n",
    "# df_articles['publishedDate'] = df_articles['publishedAt'].dt.date\n",
    "# df_articles['publishedTime'] = df_articles['publishedAt'].dt.time\n",
    "\n",
    "# # get number of sentences from string columns\n",
    "# df_articles['titleNumSents'] = df_articles['title'].apply(lambda x: len(uf.extract_sentences(x, nlp)))\n",
    "# df_articles['descNumSents'] = df_articles['description'].apply(lambda x: len(uf.extract_sentences(x, nlp)))\n",
    "# df_articles['contentNumSents'] = df_articles['content'].apply(lambda x: len(uf.extract_sentences(x, nlp)))\n",
    "\n",
    "# # get number of word tokens from string columns\n",
    "# df_articles['titleNumTokens'] = df_articles['title'].apply(lambda x: len(uf.extract_tokens(x, nlp)))\n",
    "# df_articles['descNumTokens'] = df_articles['description'].apply(lambda x: len(uf.extract_tokens(x, nlp)))\n",
    "# df_articles['contentNumTokens'] = df_articles['content'].apply(lambda x: len(uf.extract_tokens(x, nlp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_tokens(text: List[str], remove_puncs: Optional[bool] = True) -> List[str]:\n",
    "#     '''\n",
    "#     Extracts tokens from string data using NLP tokenizer with option to remove punctuations.\n",
    "    \n",
    "#     :params:\n",
    "#     text: List[Str] - given text in array\n",
    "#     remove_puncs: Bool - if True, carries out additional remove of non-alphanumeric characters\n",
    "#     '''\n",
    "#     _stripped_text = text.strip()\n",
    "#     _tokens = list(filter(lambda s: s != '', word_tokenize(_stripped_text)))\n",
    "#     if remove_puncs:\n",
    "#         _tokens = alphanum_regex.findall(' '.join(_tokens))\n",
    "#     return _tokens\n",
    "\n",
    "# def extract_sentences(sent: List[str], nlp: spacy.lang.en.English):\n",
    "#     '''\n",
    "#     Extracts sentences from string data using NLP sentencizer.\n",
    "    \n",
    "#     :params:\n",
    "#     sent: List[Str] - given text in array\n",
    "#     nlp: object - imported spacy english model\n",
    "#     '''\n",
    "#     _doc = nlp(sent)\n",
    "#     return list(_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# df = pd.read_csv(os.path.join(os.getcwd(), 'temporary_gnews_db_articles.csv'))\n",
    "# df.rename({'descNumTokens': 'descriptionNumTokens',\n",
    "#            'descNumSents': 'descriptionNumSents'}, axis=1, inplace=True)\n",
    "# sql_insert_articles(df, constraint_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "NAME = 'Science Daily'\n",
    "DOMAIN_NAME = 'https://www.sciencedaily.com'\n",
    "URL = f'{DOMAIN_NAME}/news/computers_math/artificial_intelligence/'\n",
    "headers = {\n",
    "    'User-Agent': 'Chrome/116.0.5845.110  Safari/18615.2.9.11.10'\n",
    "}\n",
    "# r = requests.get(url=URL, headers=headers)\n",
    "# r = requests.get(URL)\n",
    "# soup = BeautifulSoup(r.content, 'html5lib')\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translator = str.maketrans({chr(10): '', chr(9): ''})\n",
    "# article_titles = [e.text for e in soup.select('div[id*=\"heroes\"]')[0].select('div[class*=\"latest-head\"]')]\n",
    "# article_summaries = [e.text.translate(translator).split('—')[-1].strip() for e in soup.select('div[id*=\"heroes\"]')[0].select('div[class*=\"latest-summary\"]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_elements(soup, element_search, method):\n",
    "    match method:\n",
    "        case 'select':\n",
    "            return soup.select(element_search)\n",
    "        case 'find':\n",
    "            return soup.find(element_search)\n",
    "\n",
    "def extract_date(href):\n",
    "    url_date_str = href.split('.')[0].split('/')[-1]\n",
    "    url_date = datetime.strptime(url_date_str, '%y%m%d%H%M%S')\n",
    "    pub_date_full = url_date.strftime('%Y%m%d %H:%M:%S%z+00:00')\n",
    "    return pub_date_full\n",
    "\n",
    "def process_date(href):\n",
    "    url_date_str = href.split('.')[0].split('/')[-1]\n",
    "    url_date = datetime.strptime(url_date_str, '%y%m%d%H%M%S')\n",
    "    pub_date_full = url_date.strftime('%Y%m%d %H:%M:%S%z+00:00')\n",
    "    pub_date = url_date.strftime('%Y-%m-%d')\n",
    "    pub_time = url_date.strftime('%H:%M:%S')\n",
    "    return url_date, pub_date_full, pub_date, pub_time\n",
    "    # full_url = DOMAIN_NAME + url_part\n",
    "    \n",
    "def extract_content(url, element_search, method, headers=headers):\n",
    "    r = requests.get(url=url, headers=headers)\n",
    "    soup = BeautifulSoup(r.content, 'html5lib')\n",
    "    return locate_elements(soup, element_search, method)\n",
    "\n",
    "def process_text(soup, nature, method=None, element_search=None, translator=None):\n",
    "    match nature:\n",
    "        case 'title':\n",
    "            soup_title = locate_elements(soup, element_search, method)\n",
    "            return [e.text for e in soup_title]\n",
    "        case 'summary':\n",
    "            soup_summary = locate_elements(soup, element_search, method)\n",
    "            return [e.text.translate(translator).split('—')[-1].strip() for e in soup_summary]\n",
    "        case 'story':\n",
    "            return soup.text.translate(translator)\n",
    "        case _:\n",
    "            raise Exception('Please select correct text processing')\n",
    "\n",
    "# def process_title(soup, element_search):\n",
    "#     return [e.text for e in soup.select(element_search)]\n",
    "\n",
    "# def process_summary(soup, element_search, translator):\n",
    "#     return [e.text.translate(translator).split('—')[-1].strip() for e in soup.select('div[class*=\"latest-summary\"]')]\n",
    "\n",
    "\n",
    "### process begin\n",
    "translator = str.maketrans({chr(10): '', chr(9): ''})\n",
    "\n",
    "soup_heroes = extract_content(URL, 'div[id*=\"heroes\"]', 'select')[0]\n",
    "soup_latests = locate_elements(soup_heroes, 'div[class*=\"latest-head\"]', 'select')\n",
    "article_titles = process_text(soup_heroes, 'title', method='select', element_search='div[class*=\"latest-head\"]')\n",
    "article_summary = process_text(soup_heroes, 'story', method='select', element_search='div[class*=\"latest-summary\"]', translator=translator)\n",
    "\n",
    "story_list, url_list, url_date_list, pub_date_full_list, pub_date_list, pub_time_list = [], [], [], [], [], []\n",
    "for soup_latest in soup_latests:\n",
    "    element_search = 'a'\n",
    "    soup_latest_a = locate_elements(soup_latest, element_search, 'find')\n",
    "    href_latest = soup_latest_a['href']\n",
    "    # url_date, pub_date_full, pub_date, pub_time = process_date(href_latest)\n",
    "    pub_date_full = extract_date(href_latest)\n",
    "\n",
    "    STORY_URL = DOMAIN_NAME + href_latest\n",
    "    element_search = 'div[id=\"story_text\"]'\n",
    "    soup_story = extract_content(STORY_URL, element_search, 'select')[0]\n",
    "    article_story = process_text(soup_story, 'story', translator=translator)\n",
    "    \n",
    "    story_list.append(article_story)\n",
    "    url_list.append(STORY_URL)\n",
    "    # url_date_list.append(url_date)\n",
    "    pub_date_full_list.append(pub_date_full)\n",
    "    # pub_date_list.append(pub_date)\n",
    "    # pub_time_list.append(pub_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All articles inserted\n"
     ]
    }
   ],
   "source": [
    "df_articles_2 = pd.DataFrame({\n",
    "    'title': article_titles,\n",
    "    'description': article_summary,\n",
    "    'content': story_list,\n",
    "    'url': url_list,\n",
    "    'image': '',\n",
    "    'publishedAt': pub_date_full_list,\n",
    "    'name': NAME,\n",
    "    'domainName': DOMAIN_NAME\n",
    "    # 'publishedDate': pub_date_list,\n",
    "    # 'publishedTime': pub_time_list\n",
    "})\n",
    "\n",
    "df_articles_2 = multiextractor.process_datetime(df_articles_2)\n",
    "df_articles_2 = multiextractor.process_sentence_count(df_articles_2, nlp, 'title', 'description', 'content')\n",
    "df_articles_2 = multiextractor.process_token_count(df_articles_2, nlp, 'title', 'description', 'content')\n",
    "\n",
    "### LOAD - articles\n",
    "multiextractor.sql_insert_articles(df_articles_2, constraint_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NAME = 'Morning Brew - Tech'\n",
    "# DOMAIN_NAME = 'https://www.emergingtechbrew.com/'\n",
    "# URL = f'{DOMAIN_NAME}'\n",
    "# headers = {\n",
    "#     'User-Agent': 'Chrome/116.0.5845.110  Safari/18615.2.9.11.10'\n",
    "# }\n",
    "\n",
    "# soup_techbrew = extract_content(URL, 'div[class=\"style__GridContainer-sc-47acaf1e-1 hflwCX\"]', 'select')\n",
    "# soup_techbrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver \n",
    "# import chromedriver_autoinstaller \n",
    " \n",
    "# chromedriver_autoinstaller.install() \n",
    " \n",
    "# # Create Chromeoptions instance \n",
    "# options = webdriver.ChromeOptions() \n",
    " \n",
    "# # Adding argument to disable the AutomationControlled flag \n",
    "# options.add_argument(\"--disable-blink-features=AutomationControlled\") \n",
    " \n",
    "# # Exclude the collection of enable-automation switches \n",
    "# options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]) \n",
    " \n",
    "# # Turn-off userAutomationExtension \n",
    "# options.add_experimental_option(\"useAutomationExtension\", False) \n",
    " \n",
    "# # Setting the driver path and requesting a page \n",
    "# driver = webdriver.Chrome(options=options) \n",
    " \n",
    "# # Changing the property of the navigator value for webdriver to undefined \n",
    "# driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\") \n",
    "# driver.get(URL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import undetected_chromedriver as uc \n",
    "# from fake_useragent import UserAgent\n",
    "\n",
    "# ua = UserAgent()\n",
    "# # proxy = {'proxy': {'http': 'REMOVED', 'https': 'REMOVED'}}\n",
    " \n",
    "# options = uc.ChromeOptions() \n",
    "# options.add_argument('User-Agent={0}'.format(ua.chrome))\n",
    "\n",
    "# # options.headless = True \n",
    "# driver = uc.Chrome(options=options) \n",
    "# driver.get(URL)\n",
    "# # driver.maximize_window()\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from playwright.sync_api import sync_playwright\n",
    "# from playwright.async_api import async_playwright\n",
    "# import asyncio\n",
    "\n",
    "# async def main():\n",
    "#     # tag_selector = \"\"\"\n",
    "#     #   {\n",
    "#     #       // Returns the first element matching given selector in the root's subtree.\n",
    "#     #       query(root, selector) {\n",
    "#     #           return root.querySelector(selector);\n",
    "#     #       }\n",
    "#     #   }\"\"\"\n",
    "#     async with async_playwright() as p:\n",
    "#         # await p.selectors.register(\"tag\", tag_selector)\n",
    "#         # browser = await p.chromium.launch(headless=False)\n",
    "#         browser = await p.chromium.launch(\n",
    "#             headless=False,\n",
    "#             args=[\n",
    "#                 '--allow-insecure-localhost',\n",
    "#                 '--disable-blink-features=AutomationControlled'\n",
    "#             ]\n",
    "#         )\n",
    "#         page = await browser.new_page()\n",
    "#         await page.goto(URL)\n",
    "#         # await page.wait_for_selector('nav[class=\"style__NavigationWrapper-sc-7673406-0.bBfunh\"]')\n",
    "#         soup = BeautifulSoup(await page.content(), 'html5lib')\n",
    "#         await browser.close()\n",
    "        \n",
    "#     return soup\n",
    "\n",
    "# soup = await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'AAPL'\n",
    "ticker_prices = multiextractor.get_alphavan_data('daily_price', tickers=ticker)\n",
    "news = multiextractor.get_alphavan_data('news_sentiment', ticker)\n",
    "yield_data = multiextractor.get_alphavan_data('treasury_yield')\n",
    "inflation_data = multiextractor.get_alphavan_data('inflation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_price_data(data: dict) -> pd.DataFrame:\n",
    "#     tmp = pd.DataFrame(data['Time Series (Daily)']).T\n",
    "#     tmp['ticker'] = data['Meta Data']['2. Symbol']\n",
    "#     tmp['last_refreshed'] = data['Meta Data']['3. Last Refreshed']\n",
    "#     tmp['time_zone'] = data['Meta Data']['5. Time Zone']\n",
    "#     tmp.reset_index(names='date', inplace=True)\n",
    "#     tmp.columns = tmp.columns.str.replace('\\d+\\.', '', regex=True).str.strip()\n",
    "#     tmp['date'] = pd.to_datetime(tmp['date']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "#     tmp['last_refreshed'] = pd.to_datetime(tmp['last_refreshed']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "#     return tmp\n",
    "\n",
    "# def extract_perc_data(data: dict) -> pd.DataFrame:\n",
    "#     df = pd.DataFrame.from_dict(data)\n",
    "#     df['value'] = df['value'].apply(lambda x: float(x) if x != '.' else np.nan) / 100\n",
    "#     df['date'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "#     return df\n",
    "\n",
    "# def generate_sentiment_data_dict(defn: str) -> list[dict]:\n",
    "#     defn_parts = [parts.strip() for parts in defn.split(';')]\n",
    "#     score_bounds = [re.findall('(\\-{0,1}\\d+\\.\\d+)', part) for part in defn_parts]\n",
    "#     sentiments = [part.split(':')[-1].strip() for part in defn_parts]\n",
    "#     for i in range(len(score_bounds)):\n",
    "#         if i == 0:\n",
    "#             score_bounds[i].insert(0, 'NaN')\n",
    "#         elif i == len(score_bounds) - 1:\n",
    "#             score_bounds[i].append('NaN')\n",
    "            \n",
    "#     sent_data_list = []\n",
    "#     for bounds, sent_label in zip(score_bounds, sentiments):\n",
    "#         sent_data_list.append({\n",
    "#             'sentiment': sent_label,\n",
    "#             'lower_bound': bounds[0],\n",
    "#             'upper_bound': bounds[1]\n",
    "#         })\n",
    "#     return sent_data_list\n",
    "\n",
    "# def extract_main_article(article: dict) -> tuple[pd.DataFrame, str, str]:\n",
    "#     tmp_base = pd.DataFrame.from_dict([article])\n",
    "#     tmp_base['time_published'] = pd.to_datetime(tmp_base['time_published']).dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "#     _index_name = tmp_base['title'].values[0]\n",
    "#     _index_time = tmp_base['time_published'].values[0]\n",
    "#     del tmp_base['topics'], tmp_base['ticker_sentiment']\n",
    "#     return tmp_base, _index_name, _index_time\n",
    "\n",
    "# def extract_ticker_sentiment_topic(article: list[dict], index_name: str | None = None, index_time: str | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "#     tmp_article = copy.deepcopy(article)\n",
    "#     tmp_article['authors'] = ','.join(tmp_article['authors'])\n",
    "#     _sent = pd.DataFrame(tmp_article['ticker_sentiment'])\n",
    "#     _topics = pd.DataFrame.from_dict(tmp_article['topics'])\n",
    "    \n",
    "#     if (index_name is not None) | (index_time is not None):\n",
    "#         assert isinstance(index_name, str) & isinstance(index_time, str)\n",
    "#         _sent = _sent.assign(title=index_name, time_published=index_time)\n",
    "#         _topics = _topics.assign(title=index_name, time_published=index_time)\n",
    "#     return _sent, _topics\n",
    "\n",
    "# def insert_to_collection(db, collection_name, item, set_index=None, **kwargs):\n",
    "#     _collection = db[collection_name]\n",
    "#     if set_index is not None:\n",
    "#         unique = kwargs.get('unique', False)\n",
    "#         _collection.create_index(set_index, unique=unique)\n",
    "    \n",
    "#     if isinstance(item, list):\n",
    "#         if len(item) > 1:\n",
    "#             ordered = kwargs.get('ordered', True)\n",
    "#             _collection.insert_many(item, ordered=ordered)\n",
    "#         else:\n",
    "#             _collection.insert_one(item[0])\n",
    "#     else:\n",
    "#         _collection.insert_one(item[0])\n",
    "        \n",
    "# def check_doc_presence(db_name: object, collection_name: object, column: str, condition: dict) -> list[str]:\n",
    "#     query = {column: condition}\n",
    "#     return db_name[collection_name].find(query)\n",
    "\n",
    "# def extract_top_n(db_name: object, collection_name: object, column: str | list[str], n: int = 1) -> dict:\n",
    "#     if isinstance(column, list):\n",
    "#         sort_cols = [(c, -1) for c in column]\n",
    "#     else:\n",
    "#         sort_cols = [(column, -1)]\n",
    "#     result = db_name[collection_name].find().sort(sort_cols).limit(n)\n",
    "#     return list(result)[0]\n",
    "\n",
    "# def subset_data(df: pd.DataFrame, db_name: object, collection_name: object, column: str):\n",
    "#     top_values = extract_top_n(db_name, collection_name, column)\n",
    "#     idx = df[df['date'] == top_values['date']].index\n",
    "#     filtered_df = df.loc[:idx.values[0]-1]\n",
    "#     if filtered_df.shape[0] == 0:\n",
    "#         return None\n",
    "#     return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = multiextractor.mongodb_connection()\n",
    "db_name = multiextractor.mongodb_get_db(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_defn = news['sentiment_score_definition']\n",
    "sent_data_list = multiextractor.generate_sentiment_data_dict(score_defn)\n",
    "query_result = multiextractor.check_doc_presence(db_name, 'alphav_sentiment_reference', 'sentiment', {'$in': ['Bearish', 'Somewhat-Bearish', 'Neutral', 'Somewhat_Bullish', 'Bullish']})\n",
    "if len(list(query_result)) == 0:\n",
    "    multiextractor.insert_to_collection(db_name, 'alphav_sentiment_reference', sent_data_list, set_index=[('sentiment', 1)], unique=True, ordered=False)\n",
    "\n",
    "for article in news['feed']:\n",
    "    df_ticker_article_base, index_name, index_time = multiextractor.extract_main_article(article)\n",
    "    df_ticker_sent, df_ticker_topics = multiextractor.extract_ticker_sentiment_topic(article, index_name=index_name, index_time=index_time)\n",
    "    \n",
    "    query_result = multiextractor.check_doc_presence(db_name, 'alphav_news_main', 'title', {'$eq': df_ticker_article_base['title'].values[0]})\n",
    "    if len(list(query_result)) == 0:\n",
    "        multiextractor.insert_to_collection(db_name, 'alphav_news_main', df_ticker_article_base.to_dict(orient='records'), set_index=[('title', 1), ('time_published', 1)], unique=True, ordered=False)  \n",
    "        multiextractor.insert_to_collection(db_name, 'alphav_news_topic', df_ticker_topics.to_dict(orient='records'), set_index=[('title', 1), ('time_published', 1), ('topic', 1)], unique=True, ordered=False)\n",
    "        multiextractor.insert_to_collection(db_name, 'alphav_news_comp_sent', df_ticker_sent.to_dict(orient='records'), set_index=[('title', 1), ('time_published', 1), ('ticker', 1)], unique=True, ordered=False)\n",
    "\n",
    "df_daily_treasury = multiextractor.extract_perc_data(yield_data['data'])\n",
    "df_daily_treasury_2 = multiextractor.subset_data(df_daily_treasury, db_name, 'alphav_treasury', 'date')\n",
    "if df_daily_treasury_2 is not None:\n",
    "    multiextractor.insert_to_collection(db_name, 'alphav_treasury', df_daily_treasury_2.to_dict(orient='records'), set_index=[('date', 1)], unique=True, ordered=False)\n",
    "\n",
    "df_annual_inflation = multiextractor.extract_perc_data(inflation_data['data'])\n",
    "df_annual_inflation_2 = multiextractor.subset_data(df_annual_inflation, db_name, 'alphav_inflation', 'date')\n",
    "if df_annual_inflation_2 is not None:\n",
    "    multiextractor.insert_to_collection(db_name, 'alphav_inflation', df_annual_inflation_2.to_dict(orient='records'), set_index=[('date', 1)], unique=True, ordered=False)\n",
    "\n",
    "df_stock_prices = multiextractor.extract_price_data(ticker_prices)\n",
    "df_stock_prices_2 = multiextractor.subset_data(df_stock_prices, db_name, 'alphav_daily_price', ['date', 'ticker'])\n",
    "if df_stock_prices_2 is not None:\n",
    "    multiextractor.insert_to_collection(db_name, 'alphav_daily_price', df_stock_prices_2.to_dict(orient='records'), set_index=[('date', 1), ('last_refreshed', 1), ('ticker', 1)], unique=True, ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_name['alphav_sentiment_reference'].drop()\n",
    "# db_name['alphav_news_main'].drop()\n",
    "# db_name['alphav_news_topic'].drop()\n",
    "# db_name['alphav_news_comp_sent'].drop()\n",
    "# db_name['alphav_treasury'].drop()\n",
    "# db_name['alphav_inflation'].drop()\n",
    "# db_name['alphav_daily_price'].drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scratch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
