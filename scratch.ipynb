{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yfinance as yf\n",
    "# import feedparser\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import multiextractor\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import tldextract\n",
    "from urllib.parse import urlparse\n",
    "# from datetime import datetime, date, time\n",
    "import spacy\n",
    "# import asyncio\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from test_extract_rss import create_entry_from_rss\n",
    "# import multiprocessing\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get('https://www.cnbc.com/2023/09/27/amazon-lawsuit-protects-free-and-fair-competition-ftcs-lina-khan.html')\n",
    "# soup = BeautifulSoup(r.content)\n",
    "# # b_list = soup.find('div', {'class': 'ArticleBody-articleBody'}).find('div', {'class': 'group'}).find_all('p')\n",
    "# # b_para = ' '.join([i.text for i in b_list])\n",
    "\n",
    "# b_art_body = multiextractor.locate_elements(soup, 'div', 'find', attrs={'class': 'ArticleBody-articleBody'})\n",
    "# b_list = multiextractor.process_text(b_art_body, 'summary', 'find', 'div', attrs={'class': 'group'})\n",
    "# b_para = ' '.join(b_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # async version - need to try\n",
    "# def extract_rss_body(url, body_attrs=None, list_attrs=None) -> str:\n",
    "#     _body_kwargs, _list_kwargs = {}, {}\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content)\n",
    "    \n",
    "#     if body_attrs is not None: _body_kwargs.update({'attrs': body_attrs})\n",
    "#     _art_body = multiextractor.locate_elements(soup, 'div', 'find', **_body_kwargs)\n",
    "    \n",
    "#     if list_attrs is not None: _list_kwargs.update({'attrs': list_attrs})\n",
    "#     _body_list = multiextractor.process_text(_art_body, 'summary', 'find', 'div', **_list_kwargs)\n",
    "#     return ' '.join(_body_list)\n",
    "\n",
    "# async def create_entry_from_rss(url, executor, body_attrs=None, list_attrs=None) -> pd.DataFrame:\n",
    "#     _feeds = feedparser.parse(url)\n",
    "#     _entry_list = []\n",
    "#     loop = asyncio.get_event_loop()\n",
    "#     for _entry in _feeds.entries:\n",
    "#         _entry.body = await loop.run_in_executor(\n",
    "#             executor,\n",
    "#             extract_rss_body,\n",
    "#             _entry.link, \n",
    "#             body_attrs, \n",
    "#             list_attrs\n",
    "#         ) if (body_attrs is not None) & (list_attrs is not None) else _entry.summary\n",
    "#         _tmp = {\n",
    "#             'title': _entry.title,\n",
    "#             'description': _entry.summary,\n",
    "#             'content': _entry.body,\n",
    "#             'url': _entry.link,\n",
    "#             'image': '',\n",
    "#             'publishedAt': _entry.published,\n",
    "#             'name': tldextract.extract(_entry.link).domain.title(),\n",
    "#             'domainName': urlparse(_entry.link).netloc,\n",
    "#             'publishedDate': date(_entry.published_parsed.tm_year, _entry.published_parsed.tm_mon, _entry.published_parsed.tm_mday),\n",
    "#             'publishedTime': time(_entry.published_parsed.tm_hour, _entry.published_parsed.tm_min, _entry.published_parsed.tm_sec)\n",
    "#         }\n",
    "#         _entry_list.append(_tmp)\n",
    "#     _df = pd.DataFrame(_entry_list)\n",
    "#     _df['publishedAt'] = pd.to_datetime(_df['publishedAt']).dt.strftime('%Y%m%d %H:%M:%S%z+00:00')\n",
    "#     return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_rss_body(url, body_attrs=None, list_attrs=None) -> str:\n",
    "#     _body_kwargs, _list_kwargs = {}, {}\n",
    "#     r = requests.get(url)\n",
    "#     soup = BeautifulSoup(r.content)\n",
    "    \n",
    "#     if body_attrs is not None: _body_kwargs.update({'attrs': body_attrs})\n",
    "#     _art_body = multiextractor.locate_elements(soup, 'div', 'find', **_body_kwargs)\n",
    "    \n",
    "#     if list_attrs is not None: _list_kwargs.update({'attrs': list_attrs})\n",
    "#     _body_list = multiextractor.process_text(_art_body, 'summary', 'find', 'div', **_list_kwargs)\n",
    "#     return ' '.join(_body_list)\n",
    "\n",
    "# def create_entry_from_rss(url, body_attrs=None, list_attrs=None) -> pd.DataFrame:\n",
    "#     _feeds = feedparser.parse(url)\n",
    "#     _entry_list = []\n",
    "#     for _entry in _feeds.entries:\n",
    "#         _entry.body = extract_rss_body(_entry.link, body_attrs=body_attrs, list_attrs=list_attrs) if (body_attrs is not None) & (list_attrs is not None) else _entry.summary\n",
    "#         _tmp = {\n",
    "#             'title': _entry.title,\n",
    "#             'description': _entry.summary,\n",
    "#             'content': _entry.body,\n",
    "#             'url': _entry.link,\n",
    "#             'image': '',\n",
    "#             'publishedAt': _entry.published,\n",
    "#             'name': tldextract.extract(_entry.link).domain.title(),\n",
    "#             'domainName': urlparse(_entry.link).netloc,\n",
    "#             'publishedDate': date(_entry.published_parsed.tm_year, _entry.published_parsed.tm_mon, _entry.published_parsed.tm_mday),\n",
    "#             'publishedTime': time(_entry.published_parsed.tm_hour, _entry.published_parsed.tm_min, _entry.published_parsed.tm_sec)\n",
    "#         }\n",
    "#         _entry_list.append(_tmp)\n",
    "#     _df = pd.DataFrame(_entry_list)\n",
    "#     _df['publishedAt'] = pd.to_datetime(_df['publishedAt']).dt.strftime('%Y%m%d %H:%M:%S%z+00:00')\n",
    "#     return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnbc\n",
    "executor = ProcessPoolExecutor(8)\n",
    "\n",
    "url = 'https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910'\n",
    "# cnbc_arts = create_entry_from_rss(url, body_attrs={'class': 'ArticleBody-articleBody'}, list_attrs={'class': 'group'})\n",
    "cnbc_arts = await create_entry_from_rss(url, executor, body_attrs={'class': 'ArticleBody-articleBody'}, list_attrs={'class': 'group'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = cnbc_arts.copy()\n",
    "test1 = multiextractor.process_datetime(test1)\n",
    "test1 = multiextractor.process_sentence_count(test1, nlp, 'title', 'description', 'content')\n",
    "test1 = multiextractor.process_token_count(test1, nlp, 'title', 'description', 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trading economics\n",
    "executor = ProcessPoolExecutor(8)\n",
    "\n",
    "url = 'https://tradingeconomics.com/rss/news.aspx?i=bank+lending+rate'\n",
    "# trade_econ_blr = create_entry_from_rss(url)\n",
    "trade_econ_blr = await create_entry_from_rss(url, executor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = trade_econ_blr.copy()\n",
    "test2 = multiextractor.process_datetime(test2)\n",
    "test2 = multiextractor.process_sentence_count(test2, nlp, 'title', 'description', 'content')\n",
    "test2 = multiextractor.process_token_count(test2, nlp, 'title', 'description', 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # cnbc\n",
    "# url = 'https://search.cnbc.com/rs/search/combinedcms/view.xml?partnerId=wrss01&id=19854910'\n",
    "# feed = feedparser.parse(url)\n",
    "# for entry in feed.entries:\n",
    "#     entry.title\n",
    "#     entry.title_detail.language\n",
    "#     entry.link\n",
    "#     entry.summary\n",
    "#     entry.summary_detail.language\n",
    "#     entry.published\n",
    "#     pub_year = entry.published_parsed.tm_year\n",
    "#     pub_mth = entry.published_parsed.tm_mon\n",
    "#     pub_day = entry.published_parsed.tm_mday\n",
    "#     pub_hr = entry.published_parsed.tm_hour\n",
    "#     pub_min = entry.published_parsed.tm_min\n",
    "#     pub_sec = entry.published_parsed.tm_sec\n",
    "#     pub_wkday = entry.published_parsed.tm_wday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # tradingeconomics\n",
    "# url = 'https://tradingeconomics.com/rss/news.aspx?i=bank+lending+rate'\n",
    "# feed = feedparser.parse(url)\n",
    "# for entry in feed.entries:\n",
    "#     entry.title\n",
    "#     entry.title_detail.language\n",
    "#     entry.link\n",
    "#     entry.summary\n",
    "#     entry.published\n",
    "#     pub_year = entry.published_parsed.tm_year\n",
    "#     pub_mth = entry.published_parsed.tm_mon\n",
    "#     pub_day = entry.published_parsed.tm_mday\n",
    "#     pub_hr = entry.published_parsed.tm_hour\n",
    "#     pub_min = entry.published_parsed.tm_min\n",
    "#     pub_sec = entry.published_parsed.tm_sec\n",
    "#     pub_wkday = entry.published_parsed.tm_wday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn_params = multiextractor.DBConstLoader('cnbc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sql_insert_articles(df: pd.DataFrame, conn_params: multiextractor.DBConstLoader, table_name: str, constraint_col: str | None):\n",
    "    '''\n",
    "    Creates and executes SQL script for article insertion\n",
    "    \n",
    "    :params:\n",
    "    df: DataFrame object - table containing data to be inserted\n",
    "    constraint_col: str - column name indicating key column upon detecting duplicates\n",
    "    '''\n",
    "    conn = multiextractor.pg_connection(conn_params)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    sql_col_str = ''.join(['(\"', '\",\"'.join(df.columns), '\")'])\n",
    "    sql_val_str = ''.join(['(', ','.join(['%s'] * df.shape[1]), ')'])\n",
    "    args_str = ','.join(cur.mogrify(sql_val_str, x).decode('utf-8') for x in df.values)\n",
    "    \n",
    "    if constraint_col is None:\n",
    "        insert_script = f'INSERT INTO {table_name} {sql_col_str} VALUES {args_str}'\n",
    "    else:\n",
    "        sql_col_ups_str = ''.join(['(EXCLUDED.\"', '\", EXCLUDED.\"'.join(df.columns), '\")'])\n",
    "        update_query_template = '{} = {}'.format(sql_col_str, sql_col_ups_str)\n",
    "        insert_script = f'''\n",
    "            INSERT INTO {table_name} {sql_col_str}\n",
    "            VALUES {args_str}\n",
    "            ON CONFLICT (\"{constraint_col}\") DO\n",
    "            UPDATE SET {update_query_template}\n",
    "        '''\n",
    "    # print(insert_script)\n",
    "    try:\n",
    "        cur.execute(insert_script)\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        print('All articles inserted')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Error encountered in article insertion process')\n",
    "\n",
    "def sql_create_table(df: pd.DataFrame, conn_params: multiextractor.DBConstLoader, table_name: str = 'articles', unique_col: str | None = None):\n",
    "    '''\n",
    "    Creates and executes SQL script for article table creation\n",
    "    \n",
    "    :params:\n",
    "    df: DataFrame object - data table to extract table schema for SQL table creation query\n",
    "    table_name: str - name of table to be created in Postgres database\n",
    "    unique_col: str - name of column to set as unique index in created table \n",
    "    '''\n",
    "    table_script = pd.io.sql.get_schema(df, table_name)\n",
    "    idx = table_script.index('TABLE')\n",
    "    table_script = table_script[:idx + len('TABLE')] + ' IF NOT EXISTS' + table_script[idx + len('TABLE'):]\n",
    "\n",
    "    if unique_col is not None:\n",
    "        # to_replace_str = '\"' + unique_col + '\"'\n",
    "        to_replace_str = ','\n",
    "        idx = table_script.index(to_replace_str)\n",
    "        # len_replace_str = len(to_replace_str)\n",
    "        # table_script = table_script[: (idx + len_replace_str)] + ' UNIQUE' + table_script[(idx + len_replace_str):]\n",
    "        table_script = table_script[:idx] + ' UNIQUE' + table_script[idx:]\n",
    "\n",
    "    # print(table_script)\n",
    "    with multiextractor.pg_connection(conn_params) as conn:\n",
    "        try:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(table_script)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('Error encountered in table creation process')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sql_insert_articles() missing 1 required positional argument: 'table_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jerryli/Documents/Coding/Python Projects/P12/scratch.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jerryli/Documents/Coding/Python%20Projects/P12/scratch.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# sql_create_table(cnbc_arts, conn_params, table_name='cnbc_articles', unique_col='title')\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jerryli/Documents/Coding/Python%20Projects/P12/scratch.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sql_insert_articles(cnbc_arts, conn_params, constraint_col\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: sql_insert_articles() missing 1 required positional argument: 'table_name'"
     ]
    }
   ],
   "source": [
    "# sql_create_table(cnbc_arts, conn_params, table_name='cnbc_articles', unique_col='title')\n",
    "sql_insert_articles(cnbc_arts, conn_params, table_name='cnbc_articles', constraint_col='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "\n",
    "USER = os.getenv('COCKROACH_USER')\n",
    "PW = os.getenv('COCKROACH_PW')\n",
    "HOST = os.getenv('COCKROACH_HOST')\n",
    "PORT = os.getenv('COCKROACH_PORT')\n",
    "DB = os.getenv('COCKROACH_DB')\n",
    "conn_str = f'postgresql://{USER}:{PW}@{HOST}:{PORT}/{DB}'\n",
    "\n",
    "# conn = psycopg2.connect(conn_str)\n",
    "# with conn.cursor() as cur:\n",
    "#     cur.execute(\"SELECT now()\")\n",
    "#     res = cur.fetchall()\n",
    "#     conn.commit()\n",
    "#     print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
