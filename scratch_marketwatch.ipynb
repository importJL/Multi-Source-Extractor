{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "# chromedriver_autoinstaller.install()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installer function to handle missing packages\n",
    "def install(package):\n",
    "    print(package + ' package for Python not found, pip installing now....')\n",
    "    pip.main(['install', package])\n",
    "    print(package + ' package has been successfully installed for Python\\n Continuing Process...')\n",
    "\n",
    "# Ensure beautifulsoup4 is installed\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except:\n",
    "    install('beautifulsoup4')\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure selenium is installed\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "except:\n",
    "    install('selenium')\n",
    "finally:\n",
    "    from selenium import webdriver\n",
    "    # from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketWatchETL:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self._service = Service(\n",
    "                '/Users/jerryli/Downloads/chromedriver-mac-arm64/chromedriver'\n",
    "            )\n",
    "        self._opts = webdriver.ChromeOptions()\n",
    "        self._base_url = 'https://www.marketwatch.com/investing/stock/'\n",
    "        self._retries = 10\n",
    "        self._MW_USER = os.getenv('MARKETWATCH_USER')\n",
    "        self._MW_KEY = os.getenv('MARKETWATCH_KEY')\n",
    "        # self._service = Service('/Users/jerryli/Downloads/chromedriver-mac-arm64/chromedriver')\n",
    "        # self.ticker = ticker.lower()\n",
    "       # self._dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "        # self._opts = webdriver.ChromeOptions()\n",
    "        # self._dcap = dict(DesiredCapabilities.CHROME)\n",
    "        # self._dcap[\"phantomjs.page.settings.userAgent\"] = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        #                                                    \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        #                                                    \"Chrome/119.0.0.0 Safari/537.36\")\n",
    "        # self._base_url = 'https://www.marketwatch.com/investing/stock/'\n",
    "        # self._retries = 10\n",
    "\n",
    "    def login(self):\n",
    "        self._initiate_driver()\n",
    "        self.driver.get(self._base_url)\n",
    "        \n",
    "        close_popup_icon = self.driver.find_element(By.XPATH, \"//div[@id='cx-scrim']//div[@id='cx-scrim-wrapper']/button\")\n",
    "        close_popup_icon.click()\n",
    "        \n",
    "        profile_xpath = \"//div[@class='profile logged-out']/label[@class='btn--text btn--profile j-toggle-label']\"\n",
    "        profile_icon = self.driver.find_element(By.XPATH, profile_xpath)\n",
    "        self.driver.execute_script(\"arguments[0].click();\", profile_icon)\n",
    "        \n",
    "        options_xpath = \"//ul[@class='profile__menu j-menu-contents j-toggle--profile']/li\"\n",
    "        option_list = self.driver.find_elements(By.XPATH, options_xpath)\n",
    "        option_list[1].click()\n",
    "        \n",
    "        try:\n",
    "            # username_xpath = \"//div[@class='input-icon-container']/input[@id='username']\"\n",
    "            username_input_box = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'username'))\n",
    "            )\n",
    "            # driver.execute_script(\"arguments[0].click();\", username_input_box)\n",
    "            # username_input_box.clear()\n",
    "            username_input_box.send_keys(self._MW_USER)\n",
    "        except TimeoutException as e:\n",
    "            print(e)\n",
    "            \n",
    "        continue_xpath = \"//button[@class='solid-button continue-submit dj-btn-primary group']\"\n",
    "        continue_btn = self.driver.find_element(By.XPATH, continue_xpath)\n",
    "        self.driver.execute_script(\"arguments[0].click();\", continue_btn)\n",
    "        # continue_btn.click()\n",
    "        \n",
    "        try:\n",
    "            pw_xpath = \"//div[@id='password-login-card-container']//div[@class='input-icon-container']/input[@id='password-login-password']\"\n",
    "            pw_class = \"password dj-input w-full pr-10\"\n",
    "            pw_input_box = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, pw_xpath))\n",
    "            )\n",
    "            # self.driver.execute_script(\"arguments[0].click();\", pw_input_box)\n",
    "            # pw_input_box.clear()\n",
    "            self.driver.execute_script(\n",
    "                f\"\"\"document.getElementsByClassName(\"{pw_class}\")[0].setAttribute('value', '{self._MW_KEY}')\"\"\",\n",
    "                pw_input_box\n",
    "            )\n",
    "            # pw_input_box.send_keys(self._MW_KEY)\n",
    "        except TimeoutException as e:\n",
    "            print(e)\n",
    "            \n",
    "        signin_xpath = \"//button[@class='solid-button new-design basic-login-submit dj-btn-primary group w-full']\"\n",
    "        signin_btn = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, signin_xpath)))\n",
    "        # signin_btn = self.driver.find_element(By.XPATH, signin_xpath)\n",
    "        # self.driver.execute_script(\n",
    "        #     \"\"\"document.querySelector('button.solid-button.new-design.basic-login-submit.dj-btn-primary.group.w-full').click();\"\"\"\n",
    "        #     )\n",
    "        # self.driver.execute_script(\"arguments[0].click();\", signin_btn)\n",
    "        signin_btn.click()\n",
    "        # self.driver.set_page_load_timeout(2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cleaned_key_data_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_labels = raw_data['labels']\n",
    "        raw_values = raw_data['values']\n",
    "        i = 0\n",
    "        for raw_label in raw_labels:\n",
    "            raw_value = raw_values[i]\n",
    "            cleaned_data.update({str(raw_label.get_text()): raw_value.get_text()})\n",
    "            i += 1\n",
    "        return {ticker: cleaned_data}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cleaned_competitors_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_names = raw_data['Name']\n",
    "        raw_chgs = raw_data['Chg %']\n",
    "        raw_mcs = raw_data['Market Cap']\n",
    "        i = 0\n",
    "        for raw_name in raw_names:\n",
    "            raw_chg = raw_chgs[i]\n",
    "            raw_mc = raw_mcs[i]\n",
    "            cleaned_data.update({\n",
    "                str(raw_name): {\n",
    "                    'per_chg': raw_chg,\n",
    "                    'market_cap': raw_mc\n",
    "                }\n",
    "            })\n",
    "            i += 1\n",
    "        return {ticker: cleaned_data}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cleaned_financials_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_item = raw_data['Item']\n",
    "        raw_years = raw_data['Years']\n",
    "        raw_values = raw_data['Values']\n",
    "        \n",
    "        item_unique = sorted(set(raw_item))\n",
    "        year_unique = sorted(set(raw_years))\n",
    "\n",
    "        for item in item_unique:\n",
    "            year_val = {}\n",
    "            for year in year_unique:\n",
    "                year_val.update({year: raw_values['_'.join((item, year))]})\n",
    "            cleaned_data.update({item: year_val})\n",
    "        return {ticker: cleaned_data}\n",
    "\n",
    "    def _initiate_driver(self):\n",
    "        try:\n",
    "            if self.driver is None:\n",
    "                self.driver = webdriver.Chrome(\n",
    "                    service=self._service, \n",
    "                    options=self._opts)\n",
    "        except:\n",
    "            print('***SETUP ERROR: The PhantomJS Web Driver is either not configured or incorrectly configured!***')\n",
    "            sys.exit(1)\n",
    "            \n",
    "    def _get_site_url(self, ticker, **site_kwargs):\n",
    "        _url = self._base_url + ticker\n",
    "        if site_kwargs is not None:\n",
    "            _sub_page = ''\n",
    "            for k, v in site_kwargs.items():\n",
    "                if 'page' in k:\n",
    "                    _sub_page += '/' + str(v)\n",
    "            _url += _sub_page\n",
    "        return _url\n",
    "        # self.driver.get(_url)\n",
    "            \n",
    "    def _scrape_key_data(self):\n",
    "        raw_data_obj = {}\n",
    "        labels, values = list(), list()\n",
    "        i = 0\n",
    "        while i < self._retries:\n",
    "            try:\n",
    "                # time.sleep(3)\n",
    "                # html = self.driver.page_source\n",
    "                html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                items = soup.find_all('li', class_=\"kv__item\")\n",
    "                for item in items:\n",
    "                    labels.append(item.find('small', class_=\"label\"))\n",
    "                    values.append(item.find('span', class_=\"primary\"))\n",
    "                \n",
    "                if labels and values:\n",
    "                    raw_data_obj.update({'labels': labels})\n",
    "                    raw_data_obj.update({'values': values})\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        if i == self._retries:\n",
    "            print('Please check your internet connection!\\nUnable to connect...')\n",
    "            self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            sys.exit(1)\n",
    "        return raw_data_obj\n",
    "    \n",
    "    def _scrape_competitors(self):\n",
    "        raw_data_obj = {}\n",
    "        names, chgs, mktcap = list(), list(), list()\n",
    "        i = 0\n",
    "        while i < self._retries:\n",
    "            try:\n",
    "                # time.sleep(3)\n",
    "                # html = self.driver.page_source\n",
    "                html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                table = soup \\\n",
    "                            .find_all('div', class_=\"element element--table overflow--table Competitors\")[0] \\\n",
    "                            .find('table')\n",
    "                headers = table \\\n",
    "                            .find('thead') \\\n",
    "                            .find('tr', class_=\"table__row\") \\\n",
    "                            .find_all('th')\n",
    "                comp_name_lbl, per_chg_lbl, mkt_cap_lbl = [th.get_text() for th in headers]      \n",
    "                rows = table \\\n",
    "                        .find('tbody') \\\n",
    "                        .find_all('tr')\n",
    "                for td in rows:\n",
    "                    comp_name, per_chg, mc = [text for text in td.get_text().split('\\n') if text != '']\n",
    "                    names.append(comp_name)\n",
    "                    chgs.append(per_chg)\n",
    "                    mktcap.append(mc)\n",
    "\n",
    "                if names and chgs and mktcap:\n",
    "                    raw_data_obj.update({comp_name_lbl: names})\n",
    "                    raw_data_obj.update({per_chg_lbl: chgs})\n",
    "                    raw_data_obj.update({mkt_cap_lbl: mktcap})\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        if i == self._retries:\n",
    "            print('Please check your internet connection!\\nUnable to connect...')\n",
    "            self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            sys.exit(1)\n",
    "        return raw_data_obj\n",
    "\n",
    "    def _scrape_financials(self, mode=None):\n",
    "        def _fin_table_extraction(container, counter):\n",
    "            _raw_data_obj = {}\n",
    "            # table = soup \\\n",
    "            #             .find_all('div', class_=\"element element--table table--fixed financials\")[0] \\\n",
    "            #             .find('table')\n",
    "            table = container.find('table')\n",
    "            headers = table \\\n",
    "                        .find('thead') \\\n",
    "                        .find('tr', class_=\"table__row\") \\\n",
    "                        .find_all('th')\n",
    "            _tmp_headers = [[_text for _text in th.get_text().split('\\n') if _text != ''] for th in headers]\n",
    "            item_lbl, _, *years_lbl = list(reduce(iconcat, _tmp_headers, []))\n",
    "            rows = table \\\n",
    "                    .find('tbody') \\\n",
    "                    .find_all('tr')\n",
    "\n",
    "            for td in rows:\n",
    "                item_col_name, _, *item_vals = [text for text in td.get_text().split('\\n') if text != '']\n",
    "                \n",
    "                assert len(years_lbl) == len(item_vals), 'Length of years ({}) is not the same as number of values ({})'.format(len(years_lbl), len(item_vals))\n",
    "                for year, value in zip(years_lbl, item_vals):\n",
    "                    item_col_name_list.append(item_col_name)\n",
    "                    year_list.append(year)\n",
    "                    year_val_dict.update({item_col_name + '_' + year: value})\n",
    "\n",
    "            if item_col_name_list and year_list and year_val_dict:\n",
    "                _raw_data_obj.update({item_lbl: item_col_name_list})\n",
    "                _raw_data_obj.update({'Years': year_list})\n",
    "                _raw_data_obj.update({'Values': year_val_dict})\n",
    "            else:\n",
    "                counter += 1\n",
    "            return _raw_data_obj, counter\n",
    "                \n",
    "        temp_dict = {}\n",
    "        year_list, item_col_name_list, year_val_dict = list(), list(), dict()\n",
    "        i = 0\n",
    "        # while i < self._retries:\n",
    "        #     try:\n",
    "        # time.sleep(2)\n",
    "        # html = self.driver.page_source\n",
    "        html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        table_container = soup.find_all('div', class_=\"element element--table table--fixed financials\")\n",
    "        \n",
    "        if len(table_container) > 1:\n",
    "            for container in table_container:\n",
    "                raw_data_obj, i_new = _fin_table_extraction(container, i)\n",
    "            if len(temp_dict) == 0:\n",
    "                temp_dict.update(raw_data_obj)\n",
    "            else:\n",
    "                for k, v in temp_dict.items():\n",
    "                    print(type(v))\n",
    "                    if type(v) == list:\n",
    "                        temp_dict[k] = v + raw_data_obj[k]\n",
    "                    elif type(v) == dict:\n",
    "                        temp_dict[k] = v | raw_data_obj[k]\n",
    "                    \n",
    "        else:\n",
    "            temp_dict, i_new = _fin_table_extraction(table_container[0], i)\n",
    "            \n",
    "        # if i_new == i: break\n",
    "            # except:\n",
    "            #     i += 1\n",
    "            #     continue\n",
    "        # if i == self._retries:\n",
    "            # print('Please check your internet connection!\\nUnable to connect...')\n",
    "            # self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            # sys.exit(1)\n",
    "        return temp_dict\n",
    "        \n",
    "    def get_competitor_data(self, ticker):\n",
    "        url = self._get_site_url(ticker.lower())\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_comp_data = self._scrape_competitors()\n",
    "        return self._cleaned_competitors_object(ticker.lower(), _raw_comp_data)\n",
    "    \n",
    "    def get_stock_key_data(self, ticker):\n",
    "        url = self._get_site_url(ticker.lower())\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_key_data = self._scrape_key_data()\n",
    "        return self._cleaned_key_data_object(ticker.lower(), _raw_key_data)\n",
    "    \n",
    "    def get_stock_financials(self, ticker, mode=None):\n",
    "        pages = dict(page1='financials')\n",
    "        match mode:\n",
    "            case 'bs':\n",
    "                pages |= dict(page2='balance-sheet')\n",
    "            case 'bs_qt':\n",
    "                pages |= dict(page2='balance-sheet', page3='quarter')\n",
    "            case 'cf':\n",
    "                pages |= dict(page2='cash-flow')\n",
    "            case 'cf_qt':\n",
    "                pages |= dict(page2='cash-flow', page3='quarter')\n",
    "            case _:\n",
    "                pass\n",
    "        url = self._get_site_url(ticker.lower(), **pages)\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_comp_data = self._scrape_financials()\n",
    "        return self._cleaned_financials_object(ticker.lower(), _raw_comp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw = MarketWatchETL()\n",
    "mw.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "msft_fin = mw.get_stock_financials('MSFT')\n",
    "msft_fin_bs = mw.get_stock_financials('MSFT', mode='bs')\n",
    "msft_fin_bs_qt = mw.get_stock_financials('MSFT', mode='bs_qt')\n",
    "msft_fin_cf = mw.get_stock_financials('MSFT', mode='cf')\n",
    "msft_fin_cf_qt = mw.get_stock_financials('MSFT', mode='cf_qt')\n",
    "msft_comps = mw.get_competitor_data('MSFT')\n",
    "msft_key_data = mw.get_stock_key_data('MSFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_fin = mw.get_stock_financials('AAPL')\n",
    "aapl_fin_bs = mw.get_stock_financials('AAPL', mode='bs')\n",
    "aapl_fin_bs_qt = mw.get_stock_financials('AAPL', mode='bs_qt')\n",
    "aapl_fin_cf = mw.get_stock_financials('AAPL', mode='cf')\n",
    "aapl_fin_cf_qt = mw.get_stock_financials('AAPL', mode='cf_qt')\n",
    "aapl_comps = mw.get_competitor_data('AAPL')\n",
    "aapl_key_data = mw.get_stock_key_data('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "tmp_filename = ['msft_fin', 'msft_fin_bs', 'msft_fin_bs_qt', 'msft_fin_cf', 'msft_fin_cf_qt', 'msft_comps', 'msft_key_data']\n",
    "tmp_list = [msft_fin, msft_fin_bs, msft_fin_bs_qt, msft_fin_cf, msft_fin_cf_qt, msft_comps, msft_key_data]\n",
    "for filename, tmp in zip(tmp_filename, tmp_list):\n",
    "    with open(filename + '.json', 'w') as file:\n",
    "        json.dump(tmp, file, indent=4)\n",
    "        \n",
    "tmp_filename = ['aapl_fin', 'aapl_fin_bs', 'aapl_fin_bs_qt', 'aapl_fin_cf', 'aapl_fin_cf_qt', 'aapl_comps', 'aapl_key_data']\n",
    "tmp_list = [aapl_fin, aapl_fin_bs, aapl_fin_bs_qt, aapl_fin_cf, aapl_fin_cf_qt, aapl_comps, aapl_key_data]\n",
    "for filename, tmp in zip(tmp_filename, tmp_list):\n",
    "    with open(filename + '.json', 'w') as file:\n",
    "        json.dump(tmp, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "mw.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
