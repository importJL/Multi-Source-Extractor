{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pip\n",
    "import sys\n",
    "import signal\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from operator import iconcat\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "# chromedriver_autoinstaller.install()\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package installer function to handle missing packages\n",
    "def install(package):\n",
    "    print(package + ' package for Python not found, pip installing now....')\n",
    "    pip.main(['install', package])\n",
    "    print(package + ' package has been successfully installed for Python\\n Continuing Process...')\n",
    "\n",
    "# Ensure beautifulsoup4 is installed\n",
    "try:\n",
    "    from bs4 import BeautifulSoup\n",
    "except:\n",
    "    install('beautifulsoup4')\n",
    "    from bs4 import BeautifulSoup\n",
    "\n",
    "# Ensure selenium is installed\n",
    "try:\n",
    "    from selenium import webdriver\n",
    "except:\n",
    "    install('selenium')\n",
    "finally:\n",
    "    from selenium import webdriver\n",
    "    # from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "    from selenium.webdriver.chrome.service import Service\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.wait import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium.common.exceptions import TimeoutException, InvalidCookieDomainException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarketWatchETL:\n",
    "    def __init__(self):\n",
    "        self.driver = None\n",
    "        self._service = Service(\n",
    "                '/Users/jerryli/Downloads/chromedriver-mac-arm64/chromedriver'\n",
    "            )\n",
    "        self._opts = webdriver.ChromeOptions()\n",
    "        self._base_url = 'https://www.marketwatch.com/investing/stock/'\n",
    "        self._retries = 10\n",
    "        self._MW_USER = os.getenv('MARKETWATCH_USER')\n",
    "        self._MW_KEY = os.getenv('MARKETWATCH_KEY')\n",
    "\n",
    "    def login(self):\n",
    "        self._initiate_driver()\n",
    "        self.driver.get(self._base_url)\n",
    "        \n",
    "        close_popup_icon = self.driver.find_element(By.XPATH, \"//div[@id='cx-scrim']//div[@id='cx-scrim-wrapper']/button\")\n",
    "        close_popup_icon.click()\n",
    "        \n",
    "        profile_xpath = \"//div[@class='profile logged-out']/label[@class='btn--text btn--profile j-toggle-label']\"\n",
    "        profile_icon = self.driver.find_element(By.XPATH, profile_xpath)\n",
    "        self.driver.execute_script(\"arguments[0].click();\", profile_icon)\n",
    "        \n",
    "        options_xpath = \"//ul[@class='profile__menu j-menu-contents j-toggle--profile']/li\"\n",
    "        option_list = self.driver.find_elements(By.XPATH, options_xpath)\n",
    "        option_list[1].click()\n",
    "        \n",
    "        try:\n",
    "            # username_xpath = \"//div[@class='input-icon-container']/input[@id='username']\"\n",
    "            username_input_box = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.ID, 'username'))\n",
    "            )\n",
    "            # driver.execute_script(\"arguments[0].click();\", username_input_box)\n",
    "            # username_input_box.clear()\n",
    "            username_input_box.send_keys(self._MW_USER)\n",
    "        except TimeoutException as e:\n",
    "            print(e)\n",
    "            \n",
    "        continue_xpath = \"//button[@class='solid-button continue-submit dj-btn-primary group']\"\n",
    "        continue_btn = self.driver.find_element(By.XPATH, continue_xpath)\n",
    "        self.driver.execute_script(\"arguments[0].click();\", continue_btn)\n",
    "        # continue_btn.click()\n",
    "        \n",
    "        try:\n",
    "            pw_xpath = \"//div[@id='password-login-card-container']//div[@class='input-icon-container']/input[@id='password-login-password']\"\n",
    "            pw_class = \"password dj-input w-full pr-10\"\n",
    "            pw_input_box = WebDriverWait(self.driver, 10).until(\n",
    "                EC.presence_of_element_located((By.XPATH, pw_xpath))\n",
    "            )\n",
    "            # self.driver.execute_script(\"arguments[0].click();\", pw_input_box)\n",
    "            # pw_input_box.clear()\n",
    "            self.driver.execute_script(\n",
    "                f\"\"\"document.getElementsByClassName(\"{pw_class}\")[0].setAttribute('value', '{self._MW_KEY}')\"\"\",\n",
    "                pw_input_box\n",
    "            )\n",
    "            # pw_input_box.send_keys(self._MW_KEY)\n",
    "        except TimeoutException as e:\n",
    "            print(e)\n",
    "            \n",
    "        signin_xpath = \"//button[@class='solid-button new-design basic-login-submit dj-btn-primary group w-full']\"\n",
    "        signin_btn = WebDriverWait(self.driver, 10).until(EC.element_to_be_clickable((By.XPATH, signin_xpath)))\n",
    "        # signin_btn = self.driver.find_element(By.XPATH, signin_xpath)\n",
    "        # self.driver.execute_script(\n",
    "        #     \"\"\"document.querySelector('button.solid-button.new-design.basic-login-submit.dj-btn-primary.group.w-full').click();\"\"\"\n",
    "        #     )\n",
    "        # self.driver.execute_script(\"arguments[0].click();\", signin_btn)\n",
    "        signin_btn.click()\n",
    "        # self.driver.set_page_load_timeout(2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cleaned_key_data_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_labels = raw_data['labels']\n",
    "        raw_values = raw_data['values']\n",
    "        i = 0\n",
    "        for raw_label in raw_labels:\n",
    "            raw_value = raw_values[i]\n",
    "            cleaned_data.update({str(raw_label.get_text()): raw_value.get_text()})\n",
    "            i += 1\n",
    "        return {ticker: cleaned_data}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cleaned_competitors_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_names = raw_data['Name']\n",
    "        raw_chgs = raw_data['Chg %']\n",
    "        raw_mcs = raw_data['Market Cap']\n",
    "        i = 0\n",
    "        for raw_name in raw_names:\n",
    "            raw_chg = raw_chgs[i]\n",
    "            raw_mc = raw_mcs[i]\n",
    "            cleaned_data.update({\n",
    "                str(raw_name): {\n",
    "                    'per_chg': raw_chg,\n",
    "                    'market_cap': raw_mc\n",
    "                }\n",
    "            })\n",
    "            i += 1\n",
    "        return {ticker: cleaned_data}\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cleaned_financials_object(ticker, raw_data):\n",
    "        cleaned_data = {}\n",
    "        raw_item = raw_data['Item']\n",
    "        raw_years = raw_data['Years']\n",
    "        raw_values = raw_data['Values']\n",
    "        \n",
    "        item_unique = sorted(set(raw_item))\n",
    "        year_unique = sorted(set(raw_years))\n",
    "\n",
    "        for item in item_unique:\n",
    "            year_val = {}\n",
    "            for year in year_unique:\n",
    "                year_val.update({year: raw_values['_'.join((item, year))]})\n",
    "            cleaned_data.update({item: year_val})\n",
    "        return {ticker: cleaned_data}\n",
    "\n",
    "    def _initiate_driver(self):\n",
    "        try:\n",
    "            if self.driver is None:\n",
    "                self.driver = webdriver.Chrome(\n",
    "                    service=self._service, \n",
    "                    options=self._opts)\n",
    "        except:\n",
    "            print('***SETUP ERROR: The PhantomJS Web Driver is either not configured or incorrectly configured!***')\n",
    "            sys.exit(1)\n",
    "            \n",
    "    def _get_site_url(self, ticker, **site_kwargs):\n",
    "        _url = self._base_url + ticker\n",
    "        if site_kwargs is not None:\n",
    "            _sub_page = ''\n",
    "            for k, v in site_kwargs.items():\n",
    "                if 'page' in k:\n",
    "                    _sub_page += '/' + str(v)\n",
    "            _url += _sub_page\n",
    "        return _url\n",
    "        # self.driver.get(_url)\n",
    "            \n",
    "    def _scrape_key_data(self):\n",
    "        raw_data_obj = {}\n",
    "        labels, values = list(), list()\n",
    "        i = 0\n",
    "        while i < self._retries:\n",
    "            try:\n",
    "                # time.sleep(3)\n",
    "                # html = self.driver.page_source\n",
    "                html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                items = soup.find_all('li', class_=\"kv__item\")\n",
    "                for item in items:\n",
    "                    labels.append(item.find('small', class_=\"label\"))\n",
    "                    values.append(item.find('span', class_=\"primary\"))\n",
    "                \n",
    "                if labels and values:\n",
    "                    raw_data_obj.update({'labels': labels})\n",
    "                    raw_data_obj.update({'values': values})\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        if i == self._retries:\n",
    "            print('Please check your internet connection!\\nUnable to connect...')\n",
    "            self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            sys.exit(1)\n",
    "        return raw_data_obj\n",
    "    \n",
    "    def _scrape_competitors(self):\n",
    "        raw_data_obj = {}\n",
    "        names, chgs, mktcap = list(), list(), list()\n",
    "        i = 0\n",
    "        while i < self._retries:\n",
    "            try:\n",
    "                # time.sleep(3)\n",
    "                # html = self.driver.page_source\n",
    "                html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "                soup = BeautifulSoup(html, \"html.parser\")\n",
    "                table = soup \\\n",
    "                            .find_all('div', class_=\"element element--table overflow--table Competitors\")[0] \\\n",
    "                            .find('table')\n",
    "                headers = table \\\n",
    "                            .find('thead') \\\n",
    "                            .find('tr', class_=\"table__row\") \\\n",
    "                            .find_all('th')\n",
    "                comp_name_lbl, per_chg_lbl, mkt_cap_lbl = [th.get_text() for th in headers]      \n",
    "                rows = table \\\n",
    "                        .find('tbody') \\\n",
    "                        .find_all('tr')\n",
    "                for td in rows:\n",
    "                    comp_name, per_chg, mc = [text for text in td.get_text().split('\\n') if text != '']\n",
    "                    names.append(comp_name)\n",
    "                    chgs.append(per_chg)\n",
    "                    mktcap.append(mc)\n",
    "\n",
    "                if names and chgs and mktcap:\n",
    "                    raw_data_obj.update({comp_name_lbl: names})\n",
    "                    raw_data_obj.update({per_chg_lbl: chgs})\n",
    "                    raw_data_obj.update({mkt_cap_lbl: mktcap})\n",
    "                    break\n",
    "                else:\n",
    "                    i += 1\n",
    "            except:\n",
    "                i += 1\n",
    "                continue\n",
    "        if i == self._retries:\n",
    "            print('Please check your internet connection!\\nUnable to connect...')\n",
    "            self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            sys.exit(1)\n",
    "        return raw_data_obj\n",
    "\n",
    "    def _scrape_financials(self, mode=None):\n",
    "        def _fin_table_extraction(container, counter):\n",
    "            _raw_data_obj = {}\n",
    "            # table = soup \\\n",
    "            #             .find_all('div', class_=\"element element--table table--fixed financials\")[0] \\\n",
    "            #             .find('table')\n",
    "            table = container.find('table')\n",
    "            headers = table \\\n",
    "                        .find('thead') \\\n",
    "                        .find('tr', class_=\"table__row\") \\\n",
    "                        .find_all('th')\n",
    "            _tmp_headers = [[_text for _text in th.get_text().split('\\n') if _text != ''] for th in headers]\n",
    "            item_lbl, _, *years_lbl = list(reduce(iconcat, _tmp_headers, []))\n",
    "            rows = table \\\n",
    "                    .find('tbody') \\\n",
    "                    .find_all('tr')\n",
    "\n",
    "            for td in rows:\n",
    "                item_col_name, _, *item_vals = [text for text in td.get_text().split('\\n') if text != '']\n",
    "                \n",
    "                assert len(years_lbl) == len(item_vals), 'Length of years ({}) is not the same as number of values ({})'.format(len(years_lbl), len(item_vals))\n",
    "                for year, value in zip(years_lbl, item_vals):\n",
    "                    item_col_name_list.append(item_col_name)\n",
    "                    year_list.append(year)\n",
    "                    year_val_dict.update({item_col_name + '_' + year: value})\n",
    "\n",
    "            if item_col_name_list and year_list and year_val_dict:\n",
    "                _raw_data_obj.update({item_lbl: item_col_name_list})\n",
    "                _raw_data_obj.update({'Years': year_list})\n",
    "                _raw_data_obj.update({'Values': year_val_dict})\n",
    "            else:\n",
    "                counter += 1\n",
    "            return _raw_data_obj, counter\n",
    "                \n",
    "        temp_dict = {}\n",
    "        year_list, item_col_name_list, year_val_dict = list(), list(), dict()\n",
    "        i = 0\n",
    "        # while i < self._retries:\n",
    "        #     try:\n",
    "        # time.sleep(2)\n",
    "        # html = self.driver.page_source\n",
    "        html = self.driver.execute_script(\"return document.documentElement.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        table_container = soup.find_all('div', class_=\"element element--table table--fixed financials\")\n",
    "        \n",
    "        if len(table_container) > 1:\n",
    "            for container in table_container:\n",
    "                raw_data_obj, i_new = _fin_table_extraction(container, i)\n",
    "            if len(temp_dict) == 0:\n",
    "                temp_dict.update(raw_data_obj)\n",
    "            else:\n",
    "                for k, v in temp_dict.items():\n",
    "                    print(type(v))\n",
    "                    if type(v) == list:\n",
    "                        temp_dict[k] = v + raw_data_obj[k]\n",
    "                    elif type(v) == dict:\n",
    "                        temp_dict[k] = v | raw_data_obj[k]\n",
    "                    \n",
    "        else:\n",
    "            temp_dict, i_new = _fin_table_extraction(table_container[0], i)\n",
    "            \n",
    "        # if i_new == i: break\n",
    "            # except:\n",
    "            #     i += 1\n",
    "            #     continue\n",
    "        # if i == self._retries:\n",
    "            # print('Please check your internet connection!\\nUnable to connect...')\n",
    "            # self.driver.service.process.send_signal(signal.SIGTERM)\n",
    "            # sys.exit(1)\n",
    "        return temp_dict\n",
    "        \n",
    "    def get_competitor_data(self, ticker):\n",
    "        url = self._get_site_url(ticker.lower())\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_comp_data = self._scrape_competitors()\n",
    "        return self._cleaned_competitors_object(ticker.lower(), _raw_comp_data)\n",
    "    \n",
    "    def get_stock_key_data(self, ticker):\n",
    "        url = self._get_site_url(ticker.lower())\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_key_data = self._scrape_key_data()\n",
    "        return self._cleaned_key_data_object(ticker.lower(), _raw_key_data)\n",
    "    \n",
    "    def get_stock_financials(self, ticker, mode=None):\n",
    "        pages = dict(page1='financials')\n",
    "        match mode:\n",
    "            case 'is_qt':\n",
    "                pages |= dict(page2='income', page3='quarter')\n",
    "            case 'bs':\n",
    "                pages |= dict(page2='balance-sheet')\n",
    "            case 'bs_qt':\n",
    "                pages |= dict(page2='balance-sheet', page3='quarter')\n",
    "            case 'cf':\n",
    "                pages |= dict(page2='cash-flow')\n",
    "            case 'cf_qt':\n",
    "                pages |= dict(page2='cash-flow', page3='quarter')\n",
    "            case _:\n",
    "                pass\n",
    "        url = self._get_site_url(ticker.lower(), **pages)\n",
    "        if self.driver.current_url != url:\n",
    "            self.driver.get(url)\n",
    "        _raw_comp_data = self._scrape_financials()\n",
    "        return self._cleaned_financials_object(ticker.lower(), _raw_comp_data)\n",
    "    \n",
    "    \n",
    "def transform_financials(data, ticker, drop_col):\n",
    "    tmp = data.copy()\n",
    "    tmp_2 = tmp[ticker.lower()].apply(pd.Series).drop(drop_col, axis=1)\n",
    "    tmp_2 = tmp_2.T.reset_index().rename({'index': 'Statement Items'}, axis=1)\n",
    "    tmp_2['Company'] = ticker.upper()\n",
    "    tmp_2_cols = tmp_2.columns.tolist()\n",
    "    tmp_2_cols.insert(0, tmp_2_cols.pop())\n",
    "    tmp_2 = tmp_2[tmp_2_cols]\n",
    "    return tmp_2\n",
    "\n",
    "def transform_comps(data, ticker):\n",
    "    tmp = data.copy()\n",
    "    tmp_2 = tmp[ticker.lower()] \\\n",
    "                .apply(pd.Series) \\\n",
    "                .reset_index() \\\n",
    "                .rename({\n",
    "                    'index': 'Company', \n",
    "                    'per_chg': 'Percentage Change', \n",
    "                    'market_cap': 'Market Capitalization'\n",
    "                }, axis=1)\n",
    "    return tmp_2\n",
    "\n",
    "def transform_key_data(data):\n",
    "    tmp = data.copy()\n",
    "    tmp_2 = tmp.T.reset_index().rename({'index': 'Company'}, axis=1)\n",
    "    tmp_2['Company'] = tmp_2['Company'].str.upper()\n",
    "    return tmp_2\n",
    "\n",
    "def export_to_json(output_folder):\n",
    "    tmp_filename = ['{ticker}_fin_is', '{ticker}_fin_is_qt', '{ticker}_fin_bs', '{ticker}_fin_bs_qt', '{ticker}_fin_cf', '{ticker}_fin_cf_qt', '{ticker}_comps', '{ticker}_key_data']\n",
    "    tmp_proc_lists = [s.format(ticker='ticker') + '_proc' for s in tmp_filename]\n",
    "    tmp_list_of_lists = [globals().get(v) for v in tmp_proc_lists]\n",
    "    zipped_filename_procs = zip(tmp_filename, tmp_list_of_lists)\n",
    "    for filename_templ, data_list in zipped_filename_procs:\n",
    "        for data in data_list:\n",
    "            for ticker, tmp in data.items():\n",
    "                filename_full = filename_templ.format(ticker=ticker)\n",
    "                tmp.to_json(os.path.join(output_folder, filename_full + '.json'), orient='records')\n",
    "\n",
    "def import_to_df(ticker):\n",
    "    tmp_filename = [f'{ticker}_fin_is', f'{ticker}_fin_is_qt', f'{ticker}_fin_bs', f'{ticker}_fin_bs_qt', f'{ticker}_fin_cf', f'{ticker}_fin_cf_qt', f'{ticker}_comps', f'{ticker}_key_data']\n",
    "    ticker_fin_is, ticker_fin_is_qt, ticker_fin_bs, ticker_fin_bs_qt, ticker_fin_cf, ticker_fin_cf_qt, ticker_comps, ticker_key_data = None, None, None, None, None, None, None, None\n",
    "    for filename in tmp_filename:\n",
    "        with open(filename + '.json', 'r') as file:\n",
    "            match filename:\n",
    "                case filename if re.search('comps$', filename): ticker_comps = pd.read_json(file)\n",
    "                case filename if re.search('key_data$', filename): ticker_key_data = pd.read_json(file)\n",
    "                case filename if re.search('fin_is$', filename): ticker_fin_is = pd.read_json(file)\n",
    "                case filename if re.search('fin_is_qt$', filename): ticker_fin_is_qt = pd.read_json(file)\n",
    "                case filename if re.search('fin_bs$', filename): ticker_fin_bs = pd.read_json(file)\n",
    "                case filename if re.search('fin_bs_qt$', filename): ticker_fin_bs_qt = pd.read_json(file)\n",
    "                case filename if re.search('fin_cf$', filename): ticker_fin_cf = pd.read_json(file)\n",
    "                case filename if re.search('fin_cf_qt$', filename): ticker_fin_cf_qt = pd.read_json(file)\n",
    "    return ticker_comps, ticker_key_data, ticker_fin_is, ticker_fin_is_qt, ticker_fin_bs, ticker_fin_bs_qt, ticker_fin_cf, ticker_fin_cf_qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import symbols from MarketWatch watchlist csv output for scraping\n",
    "\n",
    "batch_size = 5\n",
    "symbols_list = list()\n",
    "with open('marketWatchWatchlistExport.csv', 'r') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    for row in reader:\n",
    "        if 'STOCK' in row['Charting Symbol']:\n",
    "            symbols_list.append(row['Symbol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mw = MarketWatchETL()\n",
    "# mw.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dump_cookies(mode, cookies=None, cookie_fname='mw_cookies_curr_sess.pkl'):\n",
    "    if mode == 'dump':\n",
    "        assert cookies is not None, 'No cookies present for saving.'\n",
    "        with open(cookie_fname, 'wb') as f:\n",
    "            pickle.dump(cookies, f)\n",
    "        print('Cookies for current session saved.')\n",
    "        return cookies\n",
    "    else:\n",
    "        assert os.path.exists(cookie_fname), 'Cookies file is not present for loading.'\n",
    "        with open(cookie_fname, 'rb') as f:\n",
    "            cookies = pickle.load(f)\n",
    "        print('Cookies for current session loaded.')\n",
    "        return cookies\n",
    "    \n",
    "def init_mw_scrapers(cookies=None, base_url=None):\n",
    "    if cookies is None:\n",
    "        scraper = MarketWatchETL()\n",
    "        scraper.login()\n",
    "        \n",
    "        time.sleep(2)\n",
    "        mw_cookies = load_dump_cookies('dump', cookies=scraper.driver.get_cookies())\n",
    "        return scraper, mw_cookies\n",
    "    else:\n",
    "        assert cookies is not None, 'No cookies for current session located.'\n",
    "        assert base_url is not None, 'No base URL provided.'\n",
    "        scraper = MarketWatchETL()\n",
    "        scraper._initiate_driver()\n",
    "\n",
    "        scraper.driver.get(base_url)\n",
    "\n",
    "        for cookie in cookies:\n",
    "            scraper.driver.add_cookie(cookie)\n",
    "        \n",
    "        close_popup_icon = WebDriverWait(scraper.driver, 10).until(EC.element_to_be_clickable((By.XPATH, \"//div[@id='cx-scrim']//div[@id='cx-scrim-wrapper']/button\")))\n",
    "        # close_popup_icon = scraper.driver.find_element(By.XPATH, \"//div[@id='cx-scrim']//div[@id='cx-scrim-wrapper']/button\")\n",
    "        close_popup_icon.click()\n",
    "        \n",
    "        scraper.driver.get(base_url)\n",
    "        return scraper\n",
    "        \n",
    "def clean_scrape_process(scrapers, cookie_fname='mw_cookies_curr_sess.pkl'):\n",
    "    if len(scrapers) > 0:\n",
    "        for s in scrapers:\n",
    "            s.driver.quit()\n",
    "            del s\n",
    "    os.remove(cookie_fname)\n",
    "    print('Scraper and cookies removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scraper_data(symbols_list, scraper):\n",
    "    _ticker_fin_is, _ticker_fin_is_qt, _ticker_fin_bs, _ticker_fin_bs_qt, _ticker_fin_cf, _ticker_fin_cf_qt, _ticker_comps, _ticker_key_data = [], [], [], [], [], [], [], []\n",
    "    proc_track = {0: 'income statement', 1: 'income statement quarterly', 2: 'balance sheet annual', 3: 'balance sheet quarterly', \n",
    "                4: 'cash flow annually', 5: 'cash flow quarterly', 6: 'competitors', 7: 'key stock data'}\n",
    "    \n",
    "    for symbol in symbols_list:\n",
    "        i = 0\n",
    "        print(f'Running extraction for {symbol}...')\n",
    "        try:\n",
    "            _ticker_fin_is.append(scraper.get_stock_financials(symbol))\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_fin_is_qt.append(scraper.get_stock_financials(symbol, mode='is_qt'))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_fin_bs.append(scraper.get_stock_financials(symbol, mode='bs'))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_fin_bs_qt.append(scraper.get_stock_financials(symbol, mode='bs_qt'))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_fin_cf.append(scraper.get_stock_financials(symbol, mode='cf'))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_fin_cf_qt.append(scraper.get_stock_financials(symbol, mode='cf_qt'))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_comps.append(scraper.get_competitor_data(symbol))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "            _ticker_key_data.append(scraper.get_stock_key_data(symbol))\n",
    "            i += 1\n",
    "            print(f'Processed {proc_track[i]}')\n",
    "        except:\n",
    "            print(f'Cannot extract for {symbol}.')\n",
    "            continue\n",
    "\n",
    "    return _ticker_fin_is, _ticker_fin_is_qt, _ticker_fin_bs, _ticker_fin_bs_qt, _ticker_fin_cf, _ticker_fin_cf_qt, _ticker_comps, _ticker_key_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cookies for current session saved.\n",
      "Running extraction for HSBC...Running extraction for AAPL...\n",
      "Running extraction for SONY...\n",
      "\n",
      "Running extraction for ORCL...\n",
      "Running extraction for ANET...\n",
      "Processed income statement\n",
      "Processed income statement\n",
      "Processed income statement\n",
      "Processed income statement\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed income statement quarterly\n",
      "Processed income statement quarterly\n",
      "Processed income statement quarterly\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annually\n",
      "Processed cash flow annually\n",
      "Processed cash flow annually\n",
      "Processed cash flow annually\n",
      "Cannot extract for HSBC.\n",
      "Running extraction for MS...\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow annually\n",
      "Processed cash flow quarterly\n",
      "Processed income statement\n",
      "Processed cash flow quarterly\n",
      "Processed competitors\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for NVDA...\n",
      "Processed key stock data\n",
      "Running extraction for DUOL...\n",
      "Processed income statement quarterly\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet annual\n",
      "Processed income statement\n",
      "Processed balance sheet quarterly\n",
      "Processed competitors\n",
      "Processed balance sheet annual\n",
      "Processed key stock data\n",
      "Running extraction for GSK...\n",
      "Processed income statement quarterly\n",
      "Processed cash flow annually\n",
      "Processed balance sheet quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet annual\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow annually\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for JPM...\n",
      "Processed balance sheet quarterly\n",
      "Processed income statement quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed income statement\n",
      "Processed cash flow annually\n",
      "Processed balance sheet annual\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for COUR...\n",
      "Processed income statement quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet annual\n",
      "Processed competitors\n",
      "Processed balance sheet quarterly\n",
      "Processed key stock data\n",
      "Running extraction for META...\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet annualProcessed cash flow annually\n",
      "\n",
      "Processed cash flow annually\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed competitors\n",
      "Processed cash flow annually\n",
      "Processed competitors\n",
      "Processed balance sheet annual\n",
      "Processed key stock data\n",
      "Running extraction for VRT...\n",
      "Processed key stock data\n",
      "Running extraction for DB...\n",
      "Processed cash flow quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet quarterly\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed cash flow annually\n",
      "Processed income statement quarterly\n",
      "Please check your internet connection!\n",
      "Unable to connect...\n",
      "Cannot extract for COUR.\n",
      "Running extraction for DELL...\n",
      "Cannot extract for DELL.\n",
      "Running extraction for MCK...\n",
      "Cannot extract for MCK.\n",
      "Processed balance sheet annual\n",
      "Processed cash flow quarterly\n",
      "Processed balance sheet annual\n",
      "Processed competitors\n",
      "Processed balance sheet quarterly\n",
      "Processed key stock data\n",
      "Running extraction for GOOG...\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annuallyProcessed cash flow annually\n",
      "\n",
      "Processed income statement\n",
      "Cannot extract for DB.\n",
      "Running extraction for BRK.B...\n",
      "Processed cash flow quarterly\n",
      "Processed income statement quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet annual\n",
      "Processed income statement quarterly\n",
      "Please check your internet connection!\n",
      "Unable to connect...\n",
      "Cannot extract for VRT.\n",
      "Running extraction for AVGO...\n",
      "Cannot extract for AVGO.\n",
      "Running extraction for MDB...\n",
      "Cannot extract for MDB.\n",
      "Running extraction for TSLA...\n",
      "Cannot extract for TSLA.\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet annual\n",
      "Processed cash flow annually\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annually\n",
      "Processed cash flow quarterly\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for AMD...\n",
      "Processed cash flow quarterly\n",
      "Processed competitors\n",
      "Processed income statement\n",
      "Processed key stock data\n",
      "Running extraction for SNOW...\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annually\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for AZN...\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow annually\n",
      "Processed income statement\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for MSFT...\n",
      "Processed income statement quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed income statement\n",
      "Processed balance sheet annual\n",
      "Processed income statement quarterly\n",
      "Please check your internet connection!\n",
      "Unable to connect...\n",
      "Cannot extract for SNOW.\n",
      "Processed balance sheet quarterly\n",
      "Processed balance sheet annual\n",
      "Processed cash flow annually\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow quarterly\n",
      "Processed cash flow annually\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for DK:NOVO.B...\n",
      "Processed cash flow quarterly\n",
      "Cannot extract for DK:NOVO.B.\n",
      "Running extraction for RPM...\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annually\n",
      "Processed cash flow quarterly\n",
      "Processed competitors\n",
      "Processed key stock data\n",
      "Running extraction for VNT...\n",
      "Processed income statement\n",
      "Processed income statement quarterly\n",
      "Processed balance sheet annual\n",
      "Processed balance sheet quarterly\n",
      "Processed cash flow annually\n",
      "Processed cash flow quarterly\n",
      "Please check your internet connection!\n",
      "Unable to connect...\n",
      "Cannot extract for VNT.\n",
      "Scraper and cookies removed.\n",
      "CPU times: user 44.7 s, sys: 3.82 s, total: 48.6 s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def run_extraction(symbols_list, num_scrapers):\n",
    "    symbol_loop_list = np.array_split(np.array(symbols_list), num_scrapers)\n",
    "    main_mw, mw_cookies = init_mw_scrapers()\n",
    "    # mw_cookies = load_dump_cookies('load')\n",
    "    # main_base_url = str(main_mw.driver.current_url)\n",
    "    main_base_url = str(main_mw._base_url)\n",
    "    \n",
    "    add_scrapers_list = []\n",
    "    try:\n",
    "        add_scrapers_list = [main_mw]\n",
    "        for _ in range(num_scrapers - 1):\n",
    "            new_scraper = init_mw_scrapers(cookies=mw_cookies, base_url=main_base_url)\n",
    "            add_scrapers_list.append(new_scraper)\n",
    "        # add_scrapers_list = [init_mw_scrapers(cookies=mw_cookies, base_url=main_base_url) for _ in range(num_scrapers - 1)]\n",
    "    except InvalidCookieDomainException as e:\n",
    "        print(e)\n",
    "        # clean_scrape_process(add_scrapers_list)\n",
    "    \n",
    "    assert len(symbol_loop_list) == len(add_scrapers_list), 'Unequal pairing of symbols list and scrapers.'\n",
    "    _ticker_fin_is_list, _ticker_fin_is_qt_list, _ticker_fin_bs_list, _ticker_fin_bs_qt_list, _ticker_fin_cf_list, _ticker_fin_cf_qt_list, _ticker_comps_list, _ticker_key_data_list = [], [], [], [], [], [], [], []\n",
    "    # for symbol_list, scraper in zip(symbol_loop_list, add_scrapers_list):\n",
    "    #     (ticker_fin_is, \n",
    "    #      ticker_fin_is_qt, \n",
    "    #      ticker_fin_bs, \n",
    "    #      ticker_fin_bs_qt, \n",
    "    #      ticker_fin_cf, \n",
    "    #      ticker_fin_cf_qt, \n",
    "    #      ticker_comps, \n",
    "    #      ticker_key_data) = extract_scraper_data(symbol_list, scraper)\n",
    "    #     _ticker_fin_is_list.append(ticker_fin_is)\n",
    "    #     _ticker_fin_is_qt_list.append(ticker_fin_is_qt)\n",
    "    #     _ticker_fin_bs_list.append(ticker_fin_bs)\n",
    "    #     _ticker_fin_bs_qt_list.append(ticker_fin_bs_qt)\n",
    "    #     _ticker_fin_cf_list.append(ticker_fin_cf)\n",
    "    #     _ticker_fin_cf_qt_list.append(ticker_fin_cf_qt)\n",
    "    #     _ticker_comps_list.append(ticker_comps)\n",
    "    #     _ticker_key_data_list.append(ticker_key_data)\n",
    "    \n",
    "    futures = []\n",
    "    with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "        for symbol_list, scraper in zip(symbol_loop_list, add_scrapers_list):\n",
    "            futures.append(executor.submit(extract_scraper_data, symbol_list, scraper))\n",
    "            \n",
    "    for future in futures:\n",
    "        (ticker_fin_is, \n",
    "         ticker_fin_is_qt, \n",
    "         ticker_fin_bs, \n",
    "         ticker_fin_bs_qt, \n",
    "         ticker_fin_cf, \n",
    "         ticker_fin_cf_qt, \n",
    "         ticker_comps, \n",
    "         ticker_key_data) = future.result()\n",
    "        _ticker_fin_is_list.append(ticker_fin_is)\n",
    "        _ticker_fin_is_qt_list.append(ticker_fin_is_qt)\n",
    "        _ticker_fin_bs_list.append(ticker_fin_bs)\n",
    "        _ticker_fin_bs_qt_list.append(ticker_fin_bs_qt)\n",
    "        _ticker_fin_cf_list.append(ticker_fin_cf)\n",
    "        _ticker_fin_cf_qt_list.append(ticker_fin_cf_qt)\n",
    "        _ticker_comps_list.append(ticker_comps)\n",
    "        _ticker_key_data_list.append(ticker_key_data)\n",
    "        \n",
    "    \n",
    "    clean_scrape_process(add_scrapers_list)\n",
    "    return _ticker_fin_is_list, _ticker_fin_is_qt_list, _ticker_fin_bs_list, _ticker_fin_bs_qt_list, _ticker_fin_cf_list, _ticker_fin_cf_qt_list, _ticker_comps_list, _ticker_key_data_list\n",
    "\n",
    "(\n",
    "    ticker_fin_is_list, \n",
    "    ticker_fin_is_qt_list, \n",
    "    ticker_fin_bs_list, \n",
    "    ticker_fin_bs_qt_list, \n",
    "    ticker_fin_cf_list, \n",
    "    ticker_fin_cf_qt_list,  \n",
    "    ticker_comps_list, \n",
    "    ticker_key_data_list\n",
    ") = run_extraction(symbols_list, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_fin_is = [item for list_ in ticker_fin_is_list for item in list_]\n",
    "ticker_fin_is_qt = [item for list_ in ticker_fin_is_qt_list for item in list_]\n",
    "ticker_fin_bs = [item for list_ in ticker_fin_bs_list for item in list_]\n",
    "ticker_fin_bs_qt = [item for list_ in ticker_fin_bs_qt_list for item in list_]\n",
    "ticker_fin_cf = [item for list_ in ticker_fin_cf_list for item in list_]\n",
    "ticker_fin_cf_qt = [item for list_ in ticker_fin_cf_qt_list for item in list_]\n",
    "ticker_comps = [item for list_ in ticker_comps_list for item in list_]\n",
    "ticker_key_data = [item for list_ in ticker_key_data_list for item in list_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker_key_data_proc = [{ticker: transform_key_data(pd.DataFrame(data))} for data in ticker_key_data for ticker in data.keys()]\n",
    "ticker_comps_proc = [{ticker: transform_comps(pd.DataFrame(data), ticker)} for data in ticker_comps for ticker in data.keys()]\n",
    "ticker_fin_is_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5-year trend')} for data in ticker_fin_is for ticker in data.keys()]\n",
    "ticker_fin_is_qt_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5- qtr trend')} for data in ticker_fin_is_qt for ticker in data.keys()]\n",
    "ticker_fin_bs_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5-year trend')} for data in ticker_fin_bs for ticker in data.keys()]\n",
    "ticker_fin_bs_qt_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5- qtr trend')} for data in ticker_fin_bs_qt for ticker in data.keys()]\n",
    "ticker_fin_cf_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5-year trend')} for data in ticker_fin_cf for ticker in data.keys()]\n",
    "ticker_fin_cf_qt_proc = [{ticker: transform_financials(pd.DataFrame(data), ticker, '5- qtr trend')} for data in ticker_fin_cf_qt for ticker in data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_to_json(os.path.join(os.getcwd(), 'ticker_outputs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
